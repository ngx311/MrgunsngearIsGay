import time
import tweepy
import requests
from urllib.parse import urlparse, parse_qs, urlunparse, urlencode
import re
import logging

# Twitter API credentials
API_KEY = 'YOUR API KEY'
API_SECRET = 'YOUR API SECRET'
ACCESS_TOKEN = 'YOUR ACCESS TOKEN'
ACCESS_TOKEN_SECRET = 'YOUR ACCESS TOKEN SECRET'
BEARER_TOKEN = 'YOUR BEARER TOKEN'

TARGET_USER = 'Mrgunsngear'

# Setup logging
logging.basicConfig(filename='tweet_monitor.log', level=logging.INFO,
                    format='%(asctime)s:%(levelname)s:%(message)s')

# Initialize Tweepy v2 Client with Bearer Token
client = tweepy.Client(bearer_token=BEARER_TOKEN)

# Initialize Tweepy OAuth1UserHandler for posting tweets
auth = tweepy.OAuth1UserHandler(
    consumer_key=API_KEY,
    consumer_secret=API_SECRET,
    access_token=ACCESS_TOKEN,
    access_token_secret=ACCESS_TOKEN_SECRET
)
api = tweepy.API(auth)

# List of affiliate parameters to remove
affiliate_params = ['utm_source', 'utm_medium', 'utm_campaign', 'avad']

def clean_affiliate_url(url):
    parsed_url = urlparse(url)
    clean_query = {k: v for k, v in parse_qs(parsed_url.query).items() if k not in affiliate_params}
    return urlunparse(parsed_url._replace(query=urlencode(clean_query, doseq=True)))

def check_and_clean_links(tweet_text):
    urls = re.findall(r'(https?://\S+)', tweet_text)
    cleaned_urls = []
    for url in urls:
        try:
            resolved_url = requests.get(url, allow_redirects=True, timeout=5).url
            cleaned_url = clean_affiliate_url(resolved_url)
            cleaned_urls.append(cleaned_url)
        except requests.RequestException as e:
            logging.error(f"Error resolving URL {url}: {e}")
    return cleaned_urls

def monitor_tweets():
    user = client.get_user(username=TARGET_USER)
    if user is None:
        logging.error(f"User {TARGET_USER} not found or API error occurred.")
        return

    user_id = user.data.id
    backoff_time = 60  # Start with a 1-minute backoff

    while True:
        try:
            # Fetch the last 5 tweets to minimize requests
            tweets = client.get_users_tweets(user_id, max_results=5, exclude=['retweets', 'replies'])
            if tweets.data:
                for tweet in tweets.data:
                    if "http" in tweet.text:
                        cleaned_urls = check_and_clean_links(tweet.text)
                        if cleaned_urls:
                            response_text = f"Mrgunsngear steals people's content. Here's a non-affiliate link instead! ðŸ˜Š\n" \
                                            + "\n".join(cleaned_urls)
                            
                            try:
                                # Check tweet length before posting
                                if len(response_text) <= 280:
                                    api.update_status(response_text)
                                    logging.info(f"Posted subtweet for tweet ID {tweet.id}")
                                else:
                                    logging.warning(f"Tweet too long for tweet ID {tweet.id}")
                            except tweepy.TweepyException as e:
                                logging.error(f"Error posting subtweet for tweet ID {tweet.id}: {e}")
                # Reset backoff time after successful fetch
                backoff_time = 60
            else:
                logging.info("No new tweets to process.")
                
            time.sleep(900)  # Wait for 15 minutes before next check to stay within API rate limits
        
        except tweepy.errors.TooManyRequests as e:
            # Extract rate limit reset time from headers if available
            reset_time = int(e.response.headers.get('x-rate-limit-reset', time.time() + 60))
            wait_time = max(0, reset_time - time.time())
            logging.info(f"Rate limit exceeded. Waiting for {wait_time} seconds until rate limit resets.")
            time.sleep(wait_time + 5)  # Add buffer time before retrying
            
        except Exception as e:
            logging.error(f"An unexpected error occurred: {e}")
            # Apply exponential backoff on general errors
            logging.info(f"Retrying after backoff of {backoff_time} seconds.")
            time.sleep(backoff_time)
            backoff_time = min(backoff_time * 2, 900)  # Cap the backoff time at 15 minutes

if __name__ == "__main__":
    try:
        monitor_tweets()
    except KeyboardInterrupt:
        logging.info("Program stopped by user.")
